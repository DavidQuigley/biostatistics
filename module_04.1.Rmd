---
title: 'Module 4.1: Probability and the P value'
author: "David Quigley"
output:
  word_document:
    toc: yes
  html_document:
    css: swiss.css
    toc: yes
---

Introduction
========================================

**This module explains:**

* Elementary Calculations with probability

* The meaning of the *P* value


Elementary calculations with probability
==========================================

A major stimulus to working out the mathematics of probability was to allow people to reason about outcomes when gambling. Probability is a quantitative measure of the chance that an event that could in theory be repeated many times will occur on any given repetition. Probability can be expressed as a percentage between 0% (no chance of occurrence) and 100% (certainty of occurrence). Probability can equivalently be expressed on a scale from 0 to 1. For some simple kinds of events, such as the flip of a fair coin, it is easy to calculate the probability of all outcomes. We can use a shorthand notation of P(an event) to mean “the probability that an event will occur”. P(an event) can be calculated by:

$\large{ \textit{P(event)} = \frac{ \textit{number of ways event can occur } } { \textit{total number of possible outcomes} } }$

Flipping a coin and seeing *heads*:

$\large{ \textit{ P(heads) } = \frac{1}{2} = 0.5 = 50 \% }$

There is one way to get heads, and two sides to the coin, so the probability of heads is 0.5 or 50%. In 10 flips of a coin, we would expect five flips to come up heads. Expectation has a formal meaning in probability, but here we mean the informal sense of what would surprise us the least. Other outcomes, such as six heads, or four heads, are possible, and not very surprising. Intuitively, we would be more surprised to see two heads in 10 flips than six heads in 10 flips. The mechanics of probability allow us to predict how likely any outcome would be with a fair coin. 

When rolling a die with six faces, we can likewise calculate the probability of rolling a *three*:

$\large{ \textit{ P(3) } = \frac{1}{6} = 0.167 = 16.7 \% }$

The probability of not rolling a three, that is to say, of rolling any of one, two, four, five, or six, is 

$\large{ \textit{ 1 - P(3) } = \frac{5}{6} = 0.83 = 83 \% }$

Independent and Dependent events
----------------------

When the probability of two distinct events do not influence each other, the event probabilities are **independent**. 

Consider rolling a die two times. The probability of obtaining a three on the first roll does not change the probability of obtaining a four on the second roll, because the die does not change. The two outcomes are independent of each other.

To obtain the probability of two independent events both occurring, we multiply their individual probabilities together. Thus, the probability of rolling a *three* and then rolling a *four* are:

$\large{ \textit{ P(3 then 4) } = \frac{1}{6} \times \frac{1}{6} = \frac{1}{36} = 0.0278 = 2.78 \% }$

On the other hand, consider drawing two cards from the same deck of cards without replacement. After drawing the first card, the deck has changed. The probability of drawing a card with the suit heart changes after the first draw: if one draws a heart on the first card, the second card is less likely to be a heart since the relative proportion of hearts left in the deck is now lower. To calculate the probability of a heart on the second draw, we need to know what happened on the first draw. The events are **dependent**, with the second probability **conditional** on the first. 


Is the coin fair?
=====================

Let us imagine someone hands you a coin and asks your opinion about whether it is fair, meaning likely to land either heads or tails equally, or if it is biased to land on one side preferentially. You test the coin by flipping it 10 times. On nine of those flips the coin comes up heads. How do you evaluate this result? We can formulate the observation in a specific way: 

If we assume the coin is fair, what is the probability of seeing at least nine heads in ten flips? 

Using the probability calculations we described earlier, we can enumerate all of the ways there are to make nine or ten heads in 10 flips, and divide that by the number of possible outcomes. Remember that order matters, so flipping “heads, tails, heads” is a different outcome from flipping “tails, heads, heads” even though each outcome has two heads and one tails.

We will work our way through a smaller number of flips and then get a general result.

Two flips
-----------

There are four possible orderings of two coin flips, so 

$\textit{ P(at least one of two flips is heads ) }$

$= \frac{( TH \textit{ or } HH )} {2^2} = \frac{( 1 + 1 )} {2^2} = \frac{2}{4} = 0.5$

Three flips
---------------

There are eight possible orderings of three coin flips, so 

$\textit{ P(at least two of three flips is heads ) }$

$= \frac{( THH \textit{ or } HTH \textit{ or } HHT \textit{ or } HHH )} {2^3} = \frac{( 3 + 1 )} {2^3} = \frac{4}{8} = 0.5$

Any number of flips
-----------------------

We can generalize these observations to higher number of flips:

$\textit{ P( at least three of four flips are heads ) }$
$= \frac{( 4 + 1 )} {2^4} = 0.3125$

$\textit{ P( at least nine of ten flips are heads ) }$
$= \frac{ ( 10 + 1 ) } {2^{10}} = 0.0107$

So if the coin were fair, we would expect to see at least nine heads in ten flips only a little over 1 out of 100 times. This seems like a fairly low probability, but our interpretation of whether this is so low as to argue the coin must be fair will require a judgement call. Probability cannot tell us definitively whether the coin is fair; it can only explain the likelihood that, if the coin were fair, we would see such an extreme result.


Key points about this demonstration
-----------------------

1. Testing “at least 9 heads” includes the more extreme case of 10 heads.
2. If 100 people took turns flipping a fair coin, we would expect one of them to see 9 or 10 heads
3. Seeing 9 heads in 10 flips does not prove the coin is biased; it is evidence against the hypothesis of a fair coin

The language of hypothesis testing
========================================

What does it mean to prove a biological hypothesis to be true? Is experimental research that investigating the natural world similar in a strong sense to what mathematicians do when they explore the bounds of mathematics? 

Thinking about what it means to prove a fact, and whether it is even possible to establish a claim about the natural world to be true, is a problem that philosophers have been struggling with for centuries. That might seem rather far removed from the day to day concerns of a molecular biologist or clinical researcher, but there are good reasons to spend some time with these ideas. 

The first, fundamental reason for considering this issue is that thinking carefully about what it means to prove a fact to be true, and considering that it might not be possible to do so, has deep implications for how we think about what we are doing day to day. It affects how we talk and think about study designs, and how we interpret results. 

The second, practical reason for thinking about proof is that grappling with this issue is the only way to understand what a *P* value is. Many experimental scientists work with *P* values and the hypothesis testing framework, and many of those scientists don’t have a clear understanding of what a *P* value actually is. They know it’s good when *P* values are low, and that they give some information about whether a result would be seen by chance. The idea behind the *P* value, and therefore behind much of a biomedical researcher’s relationship to quantitative measures of an experiment, remains somewhat counter-intuitive. It takes practice before it becomes second nature.

Induction in the natural world
----------------------------------

One formulation for the general process of the scientific method is that we make specific observations and then generalize these observations into models and even into laws. Some philosophers, such as David Hume, had a fundamental concern with this. We can think about their concern using a simple example. If we visit a lake crowded with birds day after day and only ever observe white swans, we may be tempted to make the induction that all swans are white. 

![](images/module_04_1_swans.png)

**Figure 1: a single black swan**

What is our assurance that there is no such creature as a black swan? Unlike mathematics, where one can define axioms and create world of formal proofs from those axioms, natural phenomena are not created by our hands. We may hypothesize that all swans are white, but the discovery of a single black swan disproves the idea that all swans are white. This idea-- that we can prove a hypothesis to be false, but not to be true-- is important because it is a powerful formulation of what we are doing as scientists. 

It is also how the statistical testing works. In the coin example, we accumulate evidence incompatible with the hypothesis that the coin is fair. At some point the weight of evidence that the coin is not fair becomes convincing enough, and we decide some other explanation must be more suitable. This way of thinking can be counter-intuitive because we generally think of ourselves as synthesizing observations and hypotheses to identify new truts, rather than providing evidence that ideas are not true. 

We do not prove biological facts to be true by induction. We instead attempt to falsify an another explanation. The jargon for this other explanation, the hypothesis of no effect, is the “null hypothesis”.

It is crucial to understand that accumulating evidence against a null hypothesis does not prove the alternative hypothesis is correct! There can always be many possible explanations for the outcome of a hypothesis test.


The P value defined
=========================


The *P* value is a a probability measurement. Its definition:

> The probability of observing an effect as large or larger than what was seen, if the null hypothesis were true.

We assume that the null hypothesis is true and then report the probability of seeing our actual data, if that assumption were true. We do not test the assumption that the a different hypothesis than the null is true. The probability described by *P* is conditional on the idea that the null hypothesis is correct. 

Importantly, the *P* value is not a point estimate of a single result. It reports the probability that a result as extreme or even more extreme could have been observed, if the null hypothesis were true.

Recall that we calculated a 1.07% chance if seeing 9 or 10 heads results in ten flips of a fair coin. The null hypothesis was that the coin was fair. The *P* value for this observation was 1.07%. Many people would be convinced that this is a reasonably rare event, but making a decision about what to do next will depend on many factors besides the *P* value itself. 

In thinking about the implications of a *P* value, we must bear several non-mathematical factors in mind. Among them:

1. What are the consequences of incorrectly rejecting the null hypothesis? If making an error is extremely costly, we may require stronger evidence.
2. What is the prior likelihood of the null hypothesis? In the coin flip example, we may feel differently about the coin depending on whether it came directly from the U.S. Mint or from the hands of a shady looking confidence artist. Bayesian methods, which we will not discuss in this course, allow formal reason about the prior probability of a finding and how new information impacts our belief in a hypothesis.

The P value is widely misunderstood
----------------------

The *P* value is quite tricky. It helps to keep in mind that *P* is only valid conditional on the assumption that the null hypothesis is true. *P* = 0.01 does not mean there is a 1% chance that the null hypothesis is false. 

*P* is the probability of the data if null hypothesis is true. We can write that using the conditional probability notation: 

P( data | ${H_{null}}$  )

To calculate probability of a hypothesis being true, given the data:

P( data | ${H_{null}}$  ), 

we need Bayesian statistics. 

Where did *P* < 0.05 come from?
============================================================

Now that we have a way to quantify the probability of an event, we can use that tool to reason about the outcome of scientific experiments. Sir Ronald Fisher, who invented a large chunk of modern statistics, including the *P* value, worked for a time in the evaluation of agricultural experiments. Fisher developed the *P* value as a tool for evaluating the strength of evidence against the hypothesis that investigators were trying to nullify: that is, the hypothesis that an intervention had no effect. Quoting one of his early writings gives a clue to a question that many people ask: what is so special about P < 0.05? 

>“If one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty (the 2 per cent point) or one in a hundred (the 1 per cent point). Personally, the writer prefers to set a low standard of significance at the 5 percent point... A scientific fact should be regarded as experimentally established only if a properly designed experiment rarely fails to give this level of significance.” 
(*Statistical Methods for Research Workers*)

So this is the origin of *P* < 0.05. There is no deep statistical theory that judges this level of evidence against the null hypothesis to be particularly important. Fisher’s argument was that low *P* values, in repeated experiments, were interesting and unlikely to be seen in the absence of a real effect. There are several points to make here. 

* Using Fisher’s method, a *P* value is a tool used for *post hoc* interpretation of the results of an experiment. This interpretation was continuous, with no cut-off value determining a decision point. 
* The *P* value here is used for inference, not for a decision procedure. A *P* value of 0.048 would be interpreted as nearly indistinguishable from a *P* value of 0.052.
* Fisher's interpretation of the *P* value was that it was a tool for inductive reasoning about the world. 

Summary
================================================================================

* We can use probability to calculate the chances seeing various events, given a set of assumptions about the world
* We do not prove biological facts to be true by induction.
* We gather evidence against another explanation
    + the null hypothesis
* P is the probability of observing an effect as large or larger than observed, if the null hypothesis were true.


