---
title: "Module 4.1: The normal distribution"
author: "David Quigley"
date: "September 25, 2015"
output: 
    html_document:
        toc: true
        css: swiss.css
---


*In this module, I will introduce the concept of statistical distributions. I focus on the standard normal distribution and show how q-q plots can be used to evaluate whether a distribution is normal. I introduce the Central Limit Theorem and the Law of Large Numbers*


What is a statistical distribution?
======================================================

A statistical distribution is a mathematical function. Mathematical functions precisely define a one-to-one relationship between a set of input variables and an output variable. $y = x^2$ is an example of a function. It is often useful to plot the values of a function, as its shape can tell us something about how the function behaves for different values of X:

```{r echo=FALSE, fig.height=3, fig.width=3}
library(grDevices)
vals = seq(from=-3,to=3, by=0.01)
par(mar=c(4,5,2,1))
plot( vals, vals^2, xlab="X", ylab=expression(X^2), xaxs="i",yaxs="i",las=1, type="l", lwd=3, main= expression( plain("Y=")*X^2) )
```

This function is well-defined: for all of the allowed values of X, there is exactly one value of Y that can be obtained by squaring the value of X. Distributions are well-defined functions that have an additional constraint: the area between the distribution's Y value and the zero line of the Y axis must sum to exactly one. The function $y = x^2$ is not a distribution, because the area under $y = x^2$ is infinite.

The simplest distribution has a single constant value for Y at every allowed value of X:

```{r echo=FALSE, fig.height=3, fig.width=7}
layout(matrix(1:2,1,2))
par(mar=c(4,5,2,1))
plot( 1,1, xlab="X", ylab="density", xaxs="i",yaxs="i",las=1, col="white",ylim=c(0,0.45), xlim=c(0,10), main="Unform distribution" )
lines(c(0,10), c(0.1, 0.1), lwd=3)

plot( 1,1, xlab="X", ylab="density", xaxs="i",yaxs="i",las=1, col="white",ylim=c(0,0.45), xlim=c(0,10), main="Unform distribution" )
vals = seq(from=0, to=10, by=0.01)
for(i in 1:length(vals)){
    x = vals[i]
    lines(c(x,x), c(0,0.1), col="cornflowerblue")
}
lines(c(0,10), c(0.1, 0.1), lwd=3)
```

The plot on the left is a distribution; although we sometimes refer to a distribution as a "curve", there is no rule that says a distribution cannot be a straight line. If you were to sum up the total area under the line, plotted in blue on the right side, you would find it equals exactly one. This distribution has a uniform value for Y at every X; it is therefore called the **uniform distribution**. Note that if we extended the plot to the right past 10 or to the left past 0, the area under the curve would exceed one and the curve could no longer be a distribution. Some distributions therefore will have **bounds**, values that they cannot exceed. 

The fact that distributions are used to reason about the *frequency* of events is the explanation for why their area must sum to one: the total area under the distribution is the *sum of all possible of frequency values*. Frequency values range between zero (the event never happens) and a maximum of one (or 100%), meaning the event always occurs.

What are distributions useful for?
======================================================

The standard normal distribution
--------------------------------------------------------

When we collect data, the values of a random variable will be different; by definition, they will vary. Although some processes appear to generate random noise with no pattern in the values, most data generated by sampling properties of living things follow a common frequency distribution:  most values are near the mean value, and more extreme values appear much less frequently. The rate of drop-off as we depart from the mean is not random, but has a characteristic shape:

```{r echo=FALSE, fig.height=3, fig.width=4}
par(mar=c(3,3,1,1))
vals=c(29.3,31.5,29.9,28.9,30,29.4,29.6,30.5,29.5,30,30.7,30.6,28.6,30.8,29.1,30.9,29.4,
28.8,29.7,29.1,30.7,31,29.3,28.5,28,30.8,28.1,31.1,31.3,28.6,29.5,29.6,30.6,29.4,
30.6,29.3,30.1,28.6,29.7,31.2,30.1,30.2,30.2,29.1,30,30,29.7,31.3,28.6,30.5,29.6,
29.1,29.9,29.5,29.8,31.4,29.1,31.8,31,29.4,31.7,31.4,30.6,29.4,30.6,30.7,29,29.1,
30.1,30.3,29.9,31.4,28.6,30.3,28.9,28.7,29.6,29.2,31.1,32.5,29.9,31.3,30.9,28.6,
30.3,30.8,30.1,29.8,29.5,29.4,30.8,30.4,30.3,31.6,29.9,29.3,31.1,30,29.8,29.2,30.6,
31.1,30.9,29.6,29.2,30.1,31.9,30.5,29.6,30.6,29.9,28.9,30.7,30.4,30.4,28.7,28.2,
29.4,29,29.8,30.5,31.3,29.1,29.4,30.9,29.6,27.3,30.8,29.4,30.1,30.8,28.8,30.7,
31.1,30.9,29.7,29.4,28.6,30.6,28.7,31,29.5,29.7,30,30.4,28.9,30.4,30,30.5,30.7,
27.1,29.5,30.2,32.3,28.6,31.5,32.7,29.8,27.1,30.2,29.6,28.8,29.9,29.2,30.7,31.9,
30,29.9,30.4,29.8,31.3,28.9,29.6,31.1,28.5,29.1,30.1,30.4,31.3,29.4,29.8,31.6,
30.1,30.1,30.1,30.7,31.9,28.7,31.2,29.5,30,29,30,28.9,30,28.8,31.1,29,29.3,29.9)
hist(vals, breaks=30, col="lightgrey", las=1, main="Histogram of typical data", ylim=c(0,22) )
lines( c( mean(vals), mean(vals) ), c(0, 22), col="cornflowerblue", lwd=3)
text( mean(vals)+0.5, 21, "Mean")
```

If we trace along the top of this frequency plot, we see the rough outline of a bell-shaped curve. 

As we obtain larger and larger samples of biological data, the frequency plot of the sample value distribution will tend towards a particular distribution.

```{r echo=FALSE, fig.height=4, fig.width=9}
layout(matrix(1:3,1,3))
vals = seq(from=-5,to=5, by=0.01)
par(mar=c(6,4,3,3))
hist(rnorm(1000), breaks=100, probability=TRUE, las=1, ylim=c(0,0.6), border="gray", col="gray", main="histogram vs. distribution", xlab="N=100")
points( vals, dnorm(vals), type="l", lwd=2, col="cornflowerblue" )
hist(rnorm(10000), breaks=100, probability=TRUE, las=1, ylim=c(0,0.6), border="gray", col="gray", main="histogram vs. distribution", xlab="N=1,000")
points( vals, dnorm(vals), type="l", lwd=2, col="cornflowerblue" )
hist(rnorm(100000), breaks=100, probability=TRUE, las=1, ylim=c(0,0.6), border="gray", col="gray", main="histogram vs. distribution", xlab="N=10,000")
points( vals, dnorm(vals), type="l", lwd=2, col="cornflowerblue" )
```

The distribution drawn in blue is called the "**standard normal distribution**", which almost sounds as though someone tried to think up the dullest possible name for a distribution. However, this distribution has spectacularly large importance in biostatistics. Variations on this **normal distribution** curve appear over and over again as we sample the real world: the height of 10-year-old children in Maryland, the weight of rabbits raised in captivity, etc. Not all data are normally distributed, but the normal distribution appears frequently enough in biomedical data that many statistical procedures rely on the assumption that data (or the residuals of fitting a model to the data, in the case of linear regression) are approximately normally distributed. Even though real data violate this assumption, in practice, many statistical procedures are very robust to these violations.

Although real data will never be identical to a particular distribution, the frequency distribution of real data will often be close enough to a distribution that we can use the distributon as a stand-in for the observed data. This is useful for many statistical tasks. For example, we can estimate the frequency of all possible values for the variable, as though we had an infinitely large data set. A useful consequence of using a defined function as a frequency estimate is we have a way to ask the question, "how frequently would we expect to see values of a certain magnitude, if they were generated by this distribution?" This is the fundmental question we need to answer for problems of comparison and hypothesis testing, which will be explored in module 5.


The standard normal distribution has a special and unique role as a kind of ideal histogram. The word 'standard' is derived from the fact that **the distribution has been scaled on the horizontal axis so that one unit on the horizontal axis corresponds to one standard deviation**. The standardization is important because it allows us to compare two or more samples with values on different scales to the standard normal curve; if values from sample one are exactly twice the size of values from sample two, they will have the same standardized distribution. 

Not all distributions are symmetrical around zero, but this one is; that means that its mean is zero. You can see that the Y value drops quickly to the X axis; although it never touches the X axis, it gets asymptotically close and is very, very close already when X equals -4 or 4.  

```{r echo=FALSE}
vals = seq(from=-5,to=5, by=0.01)
par(mar=c(4,5,2,1))
plot( vals, dnorm(vals), xlab="standard units", ylab="density", xaxs="i",yaxs="i",las=1, 
      type="l", lwd=3, ylim=c(0,0.45), main="Standard normal distribution" )
for(i in 1:length(vals)){
    x = vals[i]
    y = dnorm(x)
    lines(c(x,x), c(0,y), col="cornflowerblue")
}
points( vals, dnorm(vals), type="l", lwd=3 )
```


Inferring the probability of an observation from a distribution
--------------------------------------------------------

The formula that defines the standard normal distribution, which is not something you need to memorize, is rather complicated:

$\large{y = \frac{1}{\sqrt{2\pi}} e^{\frac{-x^2}{2}}}$

where *e* is the special mathematical constant that starts 2.71828, and $\pi$ is the special mathematical constant that starts out 3.141593. Since this formula is difficult to work with by hand, in the past it was common to use look-up tables to obtain the height of the standard normal curve at a given value of X. Now it is easier to use statistical packages such as R to calculate the area under this curve between two points. However, there are a few key values that are important to know:

* Between -1 and 1: lies about 0.68, or 68%.
* Between -2 and 2: lies about 0.95, or 95%.
* Between -3 and 3: lies about 0.997, or 99.7%.

```{r echo=FALSE, fig.height=4, fig.width=9}
layout(matrix(1:3,1,3))
vals = seq(from=-5,to=5, by=0.01)
par(mar=c(4,5,2,1))
plot( vals, dnorm(vals), xlab="standard units", ylab="density", xaxs="i",yaxs="i",las=1, 
      type="l", lwd=3, ylim=c(0,0.45), main="Standard normal distribution" )
for( i in seq(from=-1, to=1, by=0.01)){
    lines( c(i,i), c(0, dnorm(i)), col="cornflowerblue", lwd=2)
}
axis(1, -5:5)
points( vals, dnorm(vals), xlab="standard units", ylab="density", xaxs="i",yaxs="i",las=1, 
      type="l", lwd=3, ylim=c(0,0.45), main="Standard normal distribution" )
text(0,0.2,"68%", cex=2)

plot( vals, dnorm(vals), xlab="standard units", ylab="density", xaxs="i",yaxs="i",las=1, 
      type="l", lwd=3, ylim=c(0,0.45), main="Standard normal distribution" )
for( i in seq(from=-2, to=2, by=0.01)){
    lines( c(i,i), c(0, dnorm(i)), col="cornflowerblue", lwd=2)
}
axis(1, -5:5)

points( vals, dnorm(vals), xlab="standard units", ylab="density", xaxs="i",yaxs="i",las=1, 
      type="l", lwd=3, ylim=c(0,0.45), main="Standard normal distribution" )
text(0,0.2,"95%", cex=2)
plot( vals, dnorm(vals), xlab="standard units", ylab="density", xaxs="i",yaxs="i",las=1, 
      type="l", lwd=3, ylim=c(0,0.45), main="Standard normal distribution" )
axis(1, -5:5)
for( i in seq(from=-3, to=3, by=0.01)){
    lines( c(i,i), c(0, dnorm(i)), col="cornflowerblue", lwd=2)
}
text(0,0.18,"99.8%", cex=1.6)
points( vals, dnorm(vals), xlab="standard units", ylab="density", xaxs="i",yaxs="i",las=1, 
      type="l", lwd=3, ylim=c(0,0.45), main="Standard normal distribution" )

```

These bounds have important implications for how we interpret the likelihood of seeing data; for samples that are normally distributed, or close a normal distribution, 95% of the data will fall within two standard deviations of the mean value.




Are my data normal?
======================================================

We would sometimes like to determine how closely a sample follows a given distribution. A particular case of this problem is determining whether our data are normally distributed. One way to answer that question is to make a *quantile-quantile* plot, also called a *q-q plot*. Recall that a quantile is the percentage of values in a set that are smaller than the given value. To obtain quantiles at 0, 25%, 50%, 75%, and 100% in R one can use the *quantile()* function. One can also calculate the quantile for any given value by hand:

```{r}
V = c(4,5,6,3,7,19,8,2,12,8.5)
quantile(V)

V = sort(V)
vals_to_quantiles = function(V){
    ranks = rank(V)
    quantiles=rep(0,length(V))
    for(i in 1:length(V)){
        quantiles[i] = sum( ranks < ranks[i] )/ (length(V)-1)
    }
    quantiles*100
}
data.frame( V, Q=round(vals_to_quantiles(V), 2) )
```

When the two populations to be compared of the same size, it isn't necessary to calculate quantiles. You can construct a q-q plot by simply

1. sorting the two sets of values independently from lowest to highest
2. plotting the sorted values. 

As an example, let's pretend had 1,000 observations in your sample. I will generate these 1000 observations from a standard normal distribution using the *rnorm()* function in R, so we will expect our results to hew closely to a real standard normal distribution. Plotting these values as described produces:

```{r, fig.height=4, fig.width=4, echo=FALSE}
par(mar=c(5,5,3,3))
observed=rnorm(1000)
std_norm = rnorm(1000)
plot( sort(observed), sort(std_norm), xlab="observed (standard normal)", 
      ylab="standard normal distribution", main="q-q plot", las=1, pch=19, cex=0.5, col="#998ec3" )
abline(0,1, lwd=2, col="#11111133")
```

There is not a perfectly linear relationship between the points, but it is very close to linear. If the data were derived from similar distributions and have the same scale, they will follow the line *Y=X*. If the data were derived from simliar distributions but have different scale or localization, the q-q plot will be linear but a line fit to the data will have a slope and Y-intercept other than {1,0}. If we repeat the previous plot, but multiply all of the observed values by 10, we see:

```{r, fig.height=4, fig.width=4, echo=FALSE}
plot( 10*sort(observed), sort(std_norm), xlab="observed (10x standard normal)", pch=19, 
      ylab="standard normal distribution", main="q-q plot", las=1, cex=0.5, col="#998ec3" )
abline(0,1, lwd=2, col="#11111133")
abline(0,0.1, col="#11111133", lwd=2)
text(10,2.5, "Y = X", cex=0.75, col="#11111133")
text(-15,-2.75, cex=0.75, "Y = 0.1 * X", col="#11111133")
```

R has built-in functions *qqnorm()* and qqline() that make it easy to generate these plots without writing your own plotting functions.

By comparison, if the observed data were generated by a different distribution such as a t distribution (see section 4.2), the extreme ends of the q-q plot will deviate significantly from a straight line.

**EXAMPLE: life expectancy by country**

The average life expectancy for a person from birth is calculated for each country by the CIA (who knew!). You can see it online on the CIA World Factbook. For 2015, the estimates have the following histogram:

```{r echo=FALSE, fig.height=3, fig.width=4}
countries=c("Monaco","Japan","Singapore","Macau","San Marino","Iceland","Hong Kong","Andorra",
"Switzerland","Guernsey","Israel","Luxembourg","Australia","Italy","Sweden","Liechtenstein",
"Jersey","Canada","France","Norway","Spain","Austria","Anguilla","Netherlands","Bermuda",
"Cayman Islands","Isle of Man","New Zealand","Belgium","Finland","Ireland","Germany",
"United Kingdom","Greece","Saint Pierre and Miquelon","Malta","Faroe Islands","European Union",
"Korea, South","Taiwan","Virgin Islands","Turks and Caicos Islands","United States",
"Wallis and Futuna","Saint Helena, Ascension, and Tristan da Cunha","Gibraltar",
"Denmark","Puerto Rico","Portugal","Guam","Bahrain","Chile","Qatar","Cyprus","Czech Republic",
"Panama","British Virgin Islands","Costa Rica","Cuba","Albania","Slovenia","Curacao",
"Dominican Republic","Kuwait","Northern Mariana Islands","Argentina","Sint Maarten",
"Saint Lucia","New Caledonia","Lebanon","Poland","United Arab Emirates","Uruguay",
"Paraguay","French Polynesia","Brunei","Slovakia","Dominica","Morocco","Croatia","Algeria",
"Ecuador","Aruba","Sri Lanka","Bosnia and Herzegovina","Estonia","Antigua and Barbuda",
"Libya","Tonga","Macedonia","Georgia","West Bank","Tunisia","Hungary","Mexico","Cook Islands",
"Saint Kitts and Nevis","Colombia","China","Mauritius","Maldives","Serbia","Oman","Barbados",
"American Samoa","Solomon Islands","Saint Vincent and the Grenadines","Saudi Arabia","Romania",
"Gaza Strip","Iraq","Malaysia","Lithuania","Syria","Turkey","Venezuela","Seychelles",
"Thailand","El Salvador","Bulgaria","Armenia","Jordan","Latvia","Montserrat","Grenada",
"Egypt","Jamaica","Uzbekistan","Brazil","Peru","Samoa","Vietnam","Vanuatu","Nicaragua",
"Palau","Marshall Islands","Micronesia, Federated States of","Trinidad and Tobago",
"Belarus","Indonesia","Fiji","Bahamas, The","Azerbaijan","Greenland","Guatemala",
"Suriname","Cabo Verde","Ukraine","Iran","Honduras","Bangladesh","Kazakhstan","Russia",
"Moldova","Kyrgyzstan","Korea, North","Turkmenistan","Bhutan","Mongolia","Philippines",
"Bolivia","Belize","India","Guyana","Timor-Leste","Nepal","Pakistan","Tajikistan",
"Papua New Guinea","Nauru","Burma","Ghana","Tuvalu","Kiribati","Madagascar","Yemen",
"Gambia, The","Sao Tome and Principe","Togo","Cambodia","Laos","Comoros",
"Equatorial Guinea","Eritrea","Kenya","Sudan","Haiti","Djibouti","Mauritania","Western Sahara",
"South Africa","Tanzania","Ethiopia","Benin","Senegal","Malawi","Burundi","Guinea","Rwanda",
"Congo, Republic of the","Liberia","Cote d'Ivoire","Cameroon","Sierra Leone","Zimbabwe",
"Congo, Democratic Republic of the","Angola","Mali","Niger","Burkina Faso","Uganda",
"Botswana","Nigeria","Mozambique","Lesotho","Zambia","Gabon","Somalia",
"Central African Republic","Namibia","Swaziland","Afghanistan","Guinea-Bissau","Chad")
years = c(89.52,84.74,84.68,84.51,83.24,82.97,82.86,82.72,82.5,82.47,82.27,82.17,82.15,82.12,81.98,
81.77,81.76,81.76,81.75,81.7,81.57,81.39,81.31,81.23,81.15,81.13,81.09,81.05,80.88,80.77,
80.68,80.57,80.54,80.43,80.39,80.25,80.24,80.2,80.04,79.98,79.89,79.69,79.68,79.57,79.36,
79.28,79.25,79.25,79.16,78.98,78.73,78.61,78.59,78.51,78.48,78.47,78.46,78.4,78.39,78.13,
78.01,77.98,77.97,77.82,77.82,77.69,77.61,77.6,77.5,77.4,77.4,77.29,77,76.99,76.98,76.97,
76.88,76.79,76.71,76.61,76.59,76.56,76.56,76.56,76.55,76.47,76.33,76.26,76.04,76.02,75.95,
75.91,75.89,75.69,75.65,75.6,75.52,75.48,75.41,75.4,75.37,75.26,75.21,75.18,75.14,75.12,
75.09,75.05,74.92,74.87,74.85,74.75,74.69,74.69,74.57,74.54,74.49,74.43,74.42,74.39,74.37,
74.35,74.23,74.14,74.05,73.7,73.55,73.55,73.53,73.48,73.46,73.16,73.06,72.98,72.87,72.84,
72.62,72.59,72.48,72.45,72.43,72.2,72.2,72.1,72.02,71.97,71.85,71.57,71.15,71,70.94,70.55,
70.47,70.42,70.36,70.11,69.78,69.51,69.29,68.96,68.86,68.59,68.13,68.09,67.72,67.52,67.39,
67.39,67.03,66.75,66.29,66.18,66.16,65.81,65.55,65.18,64.6,64.58,64.51,64.14,63.88,63.85,
63.85,63.81,63.77,63.68,63.51,62.79,62.65,62.64,62.34,61.71,61.48,61.47,61.32,60.66,60.09,
60.08,59.67,58.79,58.6,58.34,57.93,57.79,57.05,56.93,55.63,55.34,55.13,55.12,54.93,54.18,
53.02,52.94,52.86,52.15,52.04,51.96,51.81,51.62,51.05,50.87,50.23,49.81)

par(mar=c(3,3,1,1))
hist(years, breaks=20, las=1, freq=FALSE, xlab="life expectancy in years", col="#f7f7f7", main="")
lines(c(mean(years), mean(years)), c(0, 1), lwd=2,  col="#67a9cf" )
text(62, 0.07, paste("mean=",round( mean(years), 1)), cex=1 )
```

There is a huge range of average lifespans, from Japan (`r round( years[countries=="Japan"],1)` years) to Chad (`r round(years[countries=="Chad"],1)`). The histogram shows that the peak of the density plot is to the right of the mean (`r round(mean(years),1)` years, show as a blue line). There is *left skew* due to a large number of countries with distressingly low life expectancies. We could ask whether these values are normally distributed using a q-q plot:

```{r echo=FALSE, fig.height=4, fig.width=4}
vals=qqnorm( years, las=1, pch=19, cex=0.5, col="#998ec3" )
abline( lm( vals$y~vals$x) )
```

From this plot, the data appear to deviate dramatically from a normal distribution. If we were to go ahead with a normal distribution assumption in this, we would be asserting that the following fit is reasonable, which seems to be a dubious assertion:

```{r echo=FALSE, fig.height=3, fig.width=4}
par(mar=c(5,5,1,1))
hist(years, breaks=20, las=1, freq=FALSE, xlab="life expectancy in years", 
     col="#f7f7f7", main="")
lines(seq(from=40, to=90, by=0.1), dnorm(seq(from=40, to=90, by=0.1), mean = mean(years), sd=sd(years)), col="cornflowerblue", lwd=2)
box()
```

**Histogram with normal( sample mean, sample SD) in blue**



Standard deviation math
--------------------------------------------------------------------------------

Since distributions describe the expected frequency of observing a particular outcome, they can be used to calculate the expected frequency with which one would see a particular observation as extreme or greater than a given value. This expectation has the underlying assumption that one could perform many repetitions of the experiment that produced the sample. We can also work in the other direction, asking what value range would we expect to see a given percent of the time. This allows us to answer questions like:

Scores on a national exam were normally distributed with a mean of 70 and a standard deviation of 5.

* What score is two standard deviations above the mean? 
    + Approximately what percentage of students will do that well?
*  Joe Schmoe scored a 82. In what percentile was his score?
* Professor Bobby Beancounter only hires students who scored at or above the 90th percentile. What score was required that year?


Two key statistical properties
======================================================


The Central Limit Theorem
----------------------------------

Unlike the standard Normal distribution, the Central Limit Theorem has a rather grand name. The Central Limit theorem, stated informally, states:

**If one generates a sufficiently large number of samples through an independent, identically distributed process, the mean of these values will be normally distributed even if the distribution generating the samples is not itself normal**.

There is a formal proof of this statement, but we can show it by example using dice. A fair die will produce all of its possible values with equal likelihood, so the distribution of the individual results will be uniform. For this example we will use a die with 20 sides, but the same uniform distribution would result from a coin flip or a six-sided die. 

```{r echo=FALSE, fig.height=3, fig.width=5}
plot( 1,1,  ylab="density", xaxs="i",yaxs="i",las=1, col="white",ylim=c(0,0.2), xlim=c(0,20), main="Unform distribution", 
      xlab="roll of a 20-sided die")
vals = seq(from=0, to=20, by=0.01)
for(i in 1:length(vals)){
    x = vals[i]
    lines(c(x,x), c(0,0.05), col="cornflowerblue")
}
lines(c(0,20), c(0.05, 0.05), lwd=3)
```

**Distribution of values from a 20-sided die**

The mean, or expected number of dots shown on the top of a fair six-sided die is $\frac{1+2+3+4+5+6}{6} = \frac{21}{6} = 3.5$. If we had a 20-sided die, by the same logic the expected value would be 10.5. 

If we randomly generate 10 rolls of a 20-sided die, the expected value will remain 10.5. The mean we *actually* observe might be greater or less than 10.5, but all things being equal we would expect 10.5. As we perform more repetitions of this process of rolling 10 times and calculating the mean value, we will observe a  *distribution of mean values*. This distribution will look like this:

```{r echo=FALSE, fig.height=6, fig.width=7}
layout(matrix(1:4,2,2,byrow=TRUE))
par(mar=c(3,3,3,0))
v=1:20
roll_means = rep(NA, 10000)
rolls = rep(0,10)
for(i in 1:10){
    for(j in 1:10){
        rolls[j] = sample(v)[1]
    }
    roll_means[i] = mean(rolls)
}
hist(roll_means, breaks=10, freq=FALSE, col="cornflowerblue", main="mean of 10 rolls, 10 reps")
box()

for(i in 1:100){
    for(j in 1:10){
        rolls[j] = sample(v)[1]
    }
    roll_means[i] = mean(rolls)
}
hist(roll_means, breaks=10, freq=FALSE, col="cornflowerblue", main="mean of 10 rolls, 100 reps")
box()

rolls = rep(0,10)
for(i in 1:1000){
    for(j in 1:10){
        rolls[j] = sample(v)[1]
    }
    roll_means[i] = mean(rolls)
}
hist(roll_means, breaks=10, freq=FALSE, col="cornflowerblue", main="mean of 10 rolls, 1,000 reps")
box()

rolls = rep(0,10)
for(i in 1:10000){
    for(j in 1:10){
        rolls[j] = sample(v)[1]
    }
    roll_means[i] = mean(rolls)
}
hist(roll_means, breaks=10, freq=FALSE, col="cornflowerblue", main="mean of 10 rolls, 10,000 reps")
box()
```

**Mean value of performing 10 rolls of a die**

As we increase the number of repetitions, the histograms look more and more like the standard normal distribution. The key thing about the Central Limit Theorem is that any distribution will produce the normally distributed mean value. The limiting normal distribution will have a mean equal to the mean of the underlying distribution, and the variance equal to the variance of the underlying distribution divided by the sample size.

This result is important because it means that the normal distribution can substitute for many other well-behaved distributions, provided we will gather sufficiently large amounts of data.

The Law of Large numbers
--------------------------------------------------------------------------------

The law of large numbers is easy to state in its intuitive form: as we gather larger samples, the mean of the samples will converge on the expected mean. For example, the mean (expected value) of a standard normal distribution is zero. If we begin sampling the results from a true standard normal distribution (using the *rnorm()* function in R), the cumulative mean of the results will converge to zero:

```{r echo=FALSE, fig.width=6, fig.height=4}

layout(matrix(1:2,2,1))
par(mar=c(2,3,3,1))
vals = rnorm(10000)
mus = rep(0, 10000)
for(i in 1:length(vals)){
    mus[i] = mean(vals[1:i])
}
plot(vals, pch=19, las=1, axes=FALSE, ylim=c(-5,5), cex=0.5, xaxs="i", yaxs="i",
     main="10,000 standard normally distributed values", col="#0000ff33" )
axis(2, seq(from=-5, to=5, by=2.5), las=1)

plot(mus, pch=19, las=1, axes=FALSE, ylim=c(-0.5, 0.5), cex=0.1, xaxs="i", yaxs="i",
     main="mean of 10,000 standard normally distributed values" )
axis(2, seq(from=-0.5, to=0.5, by=0.25), las=1)
box()
abline(0,0)
```

