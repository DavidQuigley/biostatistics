---
title: 'Module 7.1: The Binomial, Chi-squared and Fisher''s Exact tests'
author: "David Quigley"
output:
  html_document:
    css: swiss.css
    toc: yes
  word_document:
    toc: yes
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(comment = NA)
knitr::opts_chunk$set( prompt=TRUE ) 

knitr::knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar = c(2, 2, 1, 1))  # smaller margin on top and right
})

knitr::knit_hooks$set(med.mar = function(before, options, envir) {
    if (before) par(mar = c(4, 4, 1, 1))  # smaller margin on top and right
})

```

Introduction
========================================

**This module explains:**

* Methods for evaluating distributions of counts of the occurrences of categorical variables.
* The Binomial distribution, for tests where categories take two possible variables
* Contingency table tests: the chi-squared test
* Contingency table tests: Fisher's exact test

The Binomial distribution and Bionomial tests
===============================================================

Some experiments produce results that can be divided cleanly into two distinct events. A simple example of such a process is is flipping a coin: each trial produces either a heads or a tails. If the coin is fair and balanced, the expected frequency of heads is 50%. If I had the patience to flip a coin 100 times and record the results, I would be unsurprised to see 46 heads, or 52 heads, as these are not too far from the expected result of 50 heads. However, if I recorded only 17 heads in 100 flips, I would suspect that the coin was somehow rigged to come up tails. Tests that compare the observed distribution of counts against the expected distribution from some theoretical baseline allow us to quantify the probability of such an event. 

Binomial events such as a coin flip have two possible outcomes. These outcomes are frequently termed the "success" and "failure" cases. There isn't any statistical meaning to those labels: either result could arbitrarily be assigned to be the "success", regardless of whether the outcome was desirable or not. Note that the expected probability of success does not have to be 50%. The expected probability of rolling a "six" with a fair die is $\frac{1}{6}$, or 16.7%. The probability of rolling a six can be framed as a binomal test, where success is rolling a six, and failure is rolling one of the other five outcomes. 

This issue of how to frame test the harkens back to our earlier discussion of the decisions researchers make when framing an experiment measuring BMI: is it more informative to compare "six *vs.* not six", or should we test the frequency of all outcomes together? Both theoreical and practical arguments will influence your choices. 

One common use for the Binomial test is when we wish to ask whether a sample population is unbalanced in according to a binary criterion such as the relative number of males *vs.* females, or pre- *vs.* post-menopausal patients. 


The Binomial distribution
------------------------------------------------------------------------

The frequency of success in a series of *n* binomial trials has a distribution that depends on two values: *n* and the probability *p* that any given trial is a success. With *p* = 0.5, there is an equal chance of more than 20 successes or fewer than 20 successes. The most frequent result will be exactly 20 successes, which is expected; $E_{binomial}(n,p) = n \times p = 40 \times 0.5 = 20$. As an example, given a *n* of 40 trials and *p*=0.5 (meaning a 50% chance of success on any trial), the binomial distribution looks like this:

```{r echo=FALSE}
par(mar=c(4,4,2,1))
x=barplot(dbinom( 1:40, 40, 0.5), col="#ff000033", las=1, ylim=c(0,.14),
          ylab="density", xlab="number of successes", main="binomial distribution")
axis(1, at=c( x[1], x[10], x[20], x[30], x[40] ), labels=c(0, 10, 20, 30, 40) )
box()
```

**Figure 1: a binomial distribution**

Note that we can enumerate all of the possible outcomes of a binomial trial: we observe a finite number of trials, each of which has one of two possible results. The distribution of possible outcomes is therefore a discrete distribution (with steps between possible outcomes), rather than a continuous distribution like the normal distribution. Although the number of possible outcomes becomes large very quickly, for pratical purposes of computation this is rarely a problem, even when the number of trials exceeds one million.

As the number of successes gets farther from 20, the number of coin flips that could produce that event dwindles, making that result less likely by sheer chance. This is the basis for deriving a statistical judgement about whether the coin is far or not: we can calculate the frequency with which we would observe 17 heads in 100 flips of a fair coin and judge accordingly the evidence against a null hypothesis of a fair coin. In the case of 17 heads in 100 flips, $P = 1.3\times10^{-11}$, strong evidence that the coin is unlikely to be fair.

If we modify *p* to equal 0.2 or 0.7, the binomial distribution shifts:

```{r echo=FALSE}
par(mar=c(4,4,2,1))
x=barplot(dbinom( 1:40, 40, 0.5), ylim=c(0, 0.2), col="#ff000033", ylab="density", 
        xlab="number of successes" )
axis(1, at=c( x[1], x[10], x[20], x[30], x[40] ), labels=c(0, 20, 20, 30, 40) )
barplot(dbinom( 1:40, 40, 0.2), add=TRUE, col="#00ff0033")
barplot(dbinom( 1:40, 40, 0.7), add=TRUE, col="#0000ff33")
legend( 35, 0.2, c("P=0.2","P=0.5", "P=0.7"), col=c("#00ff00","#ff0000", "#0000ff"), pch=19)
```

**Figure 2: three different binomial distributions**

The probability of some number *x* successful outcomes in *n* trials is determined by the product of the probability of success in any individual trial and the number of different ways that one could order all of the outcomes to make at least *x* successes. This is written as:

$\large{ {n \choose x} p^x(1-p)^{(n-x)} }$

The ${n \choose x}$ component is read, "*n* choose *x*", and it accounts for the different possible orderings of the outcomes. The second part of the equation, $p^x(1-p)^{(n-x)}$, is the probability of a success *p(x)* occurring *x* times multiplied by the probability of a failure (1-*p*) occurring *n-x* times. Note that there is an **explicit assumption that individual trials are independent**; if that were not the case, we could not calculate their combined probability by multiplication, which assumes independence.

Generating a *P* value from Binomial tests
------------------------------------------------------------------------

Using the definition of a *P* value and the formula for a particular Binomial distribution, we can calculate exactly the likelihood of any number of successes compared to a null hypothesis. To state the null hypothesis, define values for *n* and for *p*. The probability of seeing between *x* and *n* successes in *n* trials for a given probability of success *p* can be calculated exactly, producing a P value reflecting the likelihood of seeing as extreme or more extreme of a number of successes. 

> To perform a binomial test in R, use binom.test( x, n, p )

Note that to perform a two-tailed binomial test (testing whether the observed data are either significantly larger *or* significantly smaller than the theoretial distribution, one cannot simply double the *P* value from a single-tailed test. The reason is that the binomial distribution is not symmetrical for values of *p* other than 0.5. For a concrete evidence of this, look at the green plot for the distribution of *binomial(n=40, p=0.2)* above. By default, R will calculate a two-tailed test.

To calculate *P* values for a binomial(100, 0.5) distribution in R:

```{r}
n_successes=17
n=100
p=0.5

# one-tailed binomial
binom.test( n_successes, n, p, alternative="less")$p.value 

# sum the P values for as-extreme or more-extreme values of x
P_onetail=0
for( x in 0:n_successes){
    P_onetail = P_onetail + (choose(n,x)*(p^x)*((1-p)^(n-x)))
}
P_onetail

# two-tailed binomial
binom.test( n_successes, n, p)$p.value 

# sum the P values for the other tail
P_othertail = 0
for( x in (n-n_successes):n){
    P_othertail = P_othertail + (choose(n,x)*(p^x)*((1-p)^(n-x)))
}
P_onetail + P_othertail
```

What if I my variable can take more than two values?
------------------------------------------------------------------------

Many categorical variables can take more than two values. As a simple example, given a roll of a six-sided die, is each of the six possible outcomes equally likely? If the die were weighted to produce an excess of one outcome over the other five results, the distribution would not be uniform. If I had a hypothesis about which number was rigged, I could test that number with a *p*=.167, but what if I didn't know *a priori* which number might be rigged? There is a more general version of the binomial test called the multinomial test, but we won't cover it in these lessons. The number of calculations required begins to become excessive. 


The Chi-Squared test
===============================================================

There are many cases where we would like to assess whether the observed distribution of counts is significantly different from an expected distribution, but our data take more than two values, or there is more than one variable. Some examples:

* Is a six-sided die fair?
    + One category, six values
* In a survey of 9,372 smokers, did more males than females develop lung cancer? 
    + Two categories: sex, presence of lung cancer
* In a survey of 16,000 girls, was ethnicity associated with early menarche? 
    + Two categories: race, early menarche

In the first example, we collected information on one categorical variable with six possible values. In the second and third examples, we collected two distinct categorical variables and would like to test whether the distribution of values for one variable was similar across both values of the other variable. Data from two-variable tests is usually presented in a **two-by-two contingency table**, with counts of one variable on the vertical axis and the other variable on the horizontal axis:

```{r warning=FALSE, echo=FALSE, results='hide'}
library(knitr)
```

```{r echo=FALSE, warnings=FALSE, results='asis'}
m_sm = matrix( c( 890,4070,912,3500 ), nrow=2, ncol=2 )
sm = data.frame( male=m_sm[,1], female=m_sm[,2] )
rownames(sm) = c("non-smoker", "smoker")
kable( sm )
```

Pearson's Chi-squared test (chi is the Greek letter $\chi$, pronounced to rhyme with "my", and this test is sometimes written as the $\chi^2$ test) can be used to test for significantly different distributions of counts when categorical data do not divide into two possible values in a single category. Like the Binomial test, to perform a Chi-squared test you specify an expected distribution of counts and then compare the expected distribution to what you actually observed. The size of the difference between observed and expected distributions is reflected in the test statistic. 

Deriving the chi-squared distribution
----------------------------------------

The Binomial test provides an exact calculation of the *P* value. The chi-squared test, in contrast, provides an approximation of the *P* value. Don't be put off by that: when the assumptions of the test are met, the approximation is very useful. Like all probability density functions, the chi-squared distribution is a continuous function whose area sums to one. The chi-squared distribution is obtained by summing together the squared values of one or more standard normal distributions. The number of standard normal distributions that were summed is usually written as *k*, and it corresponds to the degrees of freedom for the chi-squared distribution.

For *k* standard normal distributions *X*,

$\chi^2_k = \sum_{i=1}^kX_i^2$

The shape of the chi-squared distribution varies depending on the number of degrees of freedom.

```{r echo=FALSE, fig.height=5, fig.width=8}
par(mar=c(3,3,1,1))
nn=length( seq(from=0,to=30, by=0.01) )
plot(dchisq( seq(from=0,to=30, by=0.01), df=2), pch=19, ylim=c(0,0.5), xlim=c(0,nn),
     type="l", yaxs="i", xaxs="i", lwd=3, axes=FALSE, xlab="chi-squared statistic", ylab="density",
     main="Chi-squared distributions")
axis(2, c(0, 0.1, 0.2, 0.3, 0.4, 0.5), las=1)
axis(1, at=(nn /6 * 0:6) , labels=seq(from=0, to=30, by=5), las=1)
box()
points(dchisq( seq(from=0,to=30, by=0.01), df=4), type="l", lwd=3, col="cornflowerblue" )
points(dchisq( seq(from=0,to=30, by=0.01), df=9), type="l", lwd=3, col="orange" )
points(dchisq( seq(from=0,to=30, by=0.01), df=12), type="l", lwd=3, col="gold" )
points(dchisq( seq(from=0,to=30, by=0.01), df=15), type="l", lwd=3, col="pink" )
legend( 2500, 0.24, c("DF=2", "DF=4", "DF=9", "DF=12", "DF=15"), col=c("black", "cornflowerblue", "orange", "gold", "pink"), pch=19)
```

**Figure 3: chi-squared distributions with different degrees of freedom**

As the number of degrees of freedom increases, the shape of the chi-squared distribution changes and becomes a closer approximation to a normal distribution:

```{r echo=FALSE, fig.width=4, fig.height=4}
par(mar=c(3,4,1,1))
nn=length( seq(from=0,to=30, by=0.01) )
plot(dchisq( seq(from=0,to=30, by=0.01), df=10), pch=19, ylim=c(0,0.1), xlim=c(0,nn),
     type="l", yaxs="i", xaxs="i", lwd=3, axes=FALSE, xlab="chi-squared statistic", ylab="density",
     main="Chi-squared distributions")
axis(2, c(0, 0.05, 0.1), las=1)
axis(1, at=(nn /6 * 0:6) , labels=seq(from=0, to=30, by=5), las=1)
box()
points(dnorm( seq(from=0,to=30, by=0.01), mean=8, sd=4.47), type="l", lwd=3, col="cornflowerblue" )
legend( 1150, 0.09, c("chi-sq DF=10", "normal(8,4.47)"), col=c("black", "cornflowerblue"), pch=19)
```

```{r echo=FALSE, fig.width=4, fig.height=4}
par(mar=c(3,4,1,1))
nn=length( seq(from=0,to=60, by=0.01) )
plot(dchisq( seq(from=0,to=60, by=0.01), df=30), pch=19, ylim=c(0,0.1), xlim=c(0,nn),
     type="l", yaxs="i", xaxs="i", lwd=3, axes=FALSE, xlab="chi-squared statistic", ylab="density",
     main="Chi-squared distributions")
axis(2, c(0, 0.05, 0.1), las=1)
axis(1, at=(nn / 6 * 0:6) , labels=seq(from=0, to=60, by=10), las=1)
box()
points(dnorm( seq(from=0,to=60, by=0.01), mean=28, sd=7.74), type="l", lwd=3, col="cornflowerblue" )
legend( 1800, 0.09, c("chi-sq DF=30", "normal(28,7.74)"), col=c("black", "cornflowerblue"), pch=19)
```

````{r echo=FALSE, fig.width=4, fig.height=4}
par(mar=c(3,4,1,1))
nn=length( seq(from=0,to=100, by=0.01) )
plot(dchisq( seq(from=0,to=100, by=0.01), df=60), pch=19, ylim=c(0,0.1), xlim=c(0,nn),
     type="l", yaxs="i", xaxs="i", lwd=3, axes=FALSE, xlab="chi-squared statistic", ylab="density",
     main="Chi-squared distributions")
axis(2, c(0, 0.05, 0.1), las=1)
axis(1, at=(nn / 10 * 0:10) , labels=seq(from=0, to=100, by=10), las=1)
box()
points(dnorm( seq(from=0,to=100, by=0.01), mean=58, sd=10.95445), type="l", lwd=3, col="cornflowerblue" )
legend( 2600, 0.09, c("chi-sq DF=60", "normal(58,10.95)"), col=c("black", "cornflowerblue"), pch=19)

```

**Figure 4: chi-squared distributions begin to approximate the normal curve**


Calculating the chi-squared statistic: one variable
------------------------------------------------------------------------

In the case of the six-sided die, the null hypothesis of a fair die implies a *uniform* distribution of counts: that there is no systematic difference in the number of counts for any one of the six numbers. The expected number of counts for a uniform discrete distribution with *n* trials is $\frac{n}{\textit{number of categories}}$; for *100* throws of a fair die with six sides that value is $\frac{100}{6} = 16.67$. We cannot actually observe 0.67 occurrences of an event, but that doesn't change the mathematics. It does, however, hint at a weakness of the chi-squared distribution: when the number of observations is small, the differences between observed and expected counts can become a large enough fraction of the total number of counts to affect the statistic. 

As the difference between observed and expected counts increases, so does the chi-squared statistic. The statistic therefore reflects the diference between the observed and expected number of counts, normalized by the expected value of counts. We use the square of the difference to constrain the difference to be positive:

$\large{\sum_{i=1}^n\frac{(O_i-E_i)^2}{E_i}}$

Note that because of the $(O_i-E_i)^2$ term, the chi-squared statistic is always positive.

*Example:* we roll a die 120 times to evaluate whether it is fair and observe the following results:

```{r echo=FALSE, results='asis'}
sm = matrix(c(20,20,20,20,20,20,18,22,17,18,30,15), byrow=TRUE, ncol=6, nrow=2)
sm = data.frame( sm )
names(sm) = c("one", "two", "three", "four", "five", "six")
rownames(sm) = c("expected", "observed")
kable(sm)
```

The chi-squared statistic for these data is:
$\chi^2 = \frac{(20-18)^2}{20} + \frac{(20-22)^2}{20} + \frac{(20-17)^2}{20} + \frac{(20-18)^2}{20} + \frac{(20-30)^2}{20} + \frac{(20-15)^2}{20}$

$\chi^2 = \frac{4+4+9+4+100+25}{20}$

$\chi^2 = 7.3$

We will have one fewer degree of freedom than the number of observations, because if we were given values for all but one of the observations plus the total *n*, we could calculate the last observation without seeing it.

> To perform a chi-squared test in R, use the chisqu.test() function

Calculating the chi-squared statistic: two variables
------------------------------------------------------------------------

If there are two variables, we can arrange the variables and their possible values into a contingency table. To calculate the expected value for each element of the table, make the assumption that if there were no difference in the distribution of counts across the categories, each element would take on a value proportional to the number of counts in each of the two variables. The proportion of elements in *n* total observations that have value *i* is $p_i=\frac{O_i}{n}$. This is simply the number of counts in a category, without sub-dividing by the other category, divided by the total number of counts. The expected value for each element is then $N p_i p_j$

```{r echo=FALSE, warnings=FALSE, results='asis'}

sm = data.frame( male=c(m_sm[1,1],m_sm[2,1],sum(m_sm[,1]) ), 
                 female=c(m_sm[1,2],m_sm[2,2],sum(m_sm[,2]) ), 
                 sum=c( sum(m_sm[1,]), sum(m_sm[2,]), sum(m_sm) ) )
rownames(sm) = c("non-smoker", "smoker", "sum")
kable( sm )
```

There were 9,372 observations in the entire experiment. Here $P_{smoker} = \frac{4070+3500}{9372} = \frac{7570}{9372} = 0.81$, and 
$P_{male} = \frac{4070+890}{9372} = \frac{4960}{9372} = 0.53$. 

This makes the expected values of the distribution:
```{r echo=FALSE, as.is=TRUE}
N = sum(m_sm)
prop_male = sum( m_sm[,1] ) / N
prop_female = 1-prop_male
prop_nonsmoker = sum( m_sm[1,] ) / N
prop_smoker = 1-prop_nonsmoker
sm_ex = data.frame( round( matrix( c( prop_male*prop_nonsmoker*N, prop_female*prop_nonsmoker*N,
                prop_male*prop_smoker*N, prop_female*prop_smoker*N ), nrow=2, ncol=2, byrow=TRUE ), 1) )
names(sm_ex)=c("male", "female")
rownames(sm_ex) = c("non-smoker", "smoker")
kable(sm_ex)
```

In the same manner as with the one-category test, the chi-squared statistic is the sum of the squared deviations of observed from expected, divided by the expected values:

$\large{\chi^2 = \sum_{i=1}^{row} \sum_{j=1}^{col}\frac{ (O_{i,j} - E_{i,j})^2}{E_{i,j}}}$

The two sum symbols next to each other just mean to sum in both directions. The value works out to `r sum( ( ( (sm[1:2,1:2]-sm_ex) )^2 ) / sm_ex )`

The number of degrees of freedom for the test is equal to (# rows-1) * (# columns-1); for a 2x2 contingency table, this value is 1.


Calculating a P value for the chi-squared statistic
------------------------------------------------------------------------

As noted above, the shape of the chi-squared distribution varies depending on the number of degrees of freedom. For two-by-two contingency tables this value will be 1, but many other possible values exist. For a one-variable test with five categories, there would be four degrees of freedom.

The primary intuition is that for a given value of the chi-squared statistic, the larger the number of degrees of freedom (the larger the number of categories), the more frequently we would expect to see this result by chance. For a chi-squared statistic of 9:

```{r echo=FALSE, results='asis'}
dens = rep(0,4)
dfs=c(3,5,7,9,11)
for(i in 1:5){
    dens[i] = 1-pchisq(9, dfs[i])
}
kable( data.frame( degrees_of_freedom=dfs, P=round(dens, 4) ) )
```


limitations and assumptions of the chi-squared test
------------------------------------------------------

The chi-squared test makes an approximation to the chi-squared distribution that is more close to exactly correct as the number of observations increases. In many cases the chi-squared test can be applied, but it bears several limitations:

* Individual observations are assumed to be independent of each other
* Individual cells must contain sufficient counts. 
    + As a conservative rule of thumb, do not use the chi-squared test for two-variable tests if any cell contains fewer than 10 counts, or for one variable tests if any cell contains fewer than 5 counts.

When the chi-squared test is inappropriate due to small sample sizes or highly unequal cell distribution, one can instead use Fisher's exact test.

Fisher's exact test
=====================

Fisher's Exact test was named for its inventor Sir Ronald Fisher, a statistian who invented much of inferential statistics in the early part of the last century. Like the Binomial test, and in contrast to the chi-squared test, Fisher's exact test provides an exact P value rather than an approximation. 

Fisher's exact test is almost always applied to two-by-two contingency data, but there are extensions that allow the test to be applied to cases with more than two categories per variable.

We will state without proof that the probability of obtaining values {a,b,c,d} in a contingency table with the form

```{r echo=FALSE}
fm = data.frame(matrix( c('a','c','b','d'), nrow=2, ncol=2))
names(fm) = c("men","women")
rownames(fm)=c("non-smoker", "smoker")
kable(fm)
```

can be calculated using the Hypergeometric distribution:

$\large{p = \frac{ {{a+b}\choose{a}} {{c+d}\choose{c}} } { n \choose{ a+c} } }$

> Fisher's exact test can be performed in R using the fisher.test() function. 

Calculating P values with Fisher's exact test
------------------------------------------------------------------

Calculating P values with Fisher's exact test is tricky because you need to identify the probability of distributions that are as extreme or more extreme than the one you observed, and this may be cumbersome or non-obvious. However, given a fixed N, you can enumerate through all possible allocations of four cells. To calculate a P value for Fisher's exact test, R reports the sum of all P values as small or smaller than the P value for the observed distribution.

R will warn you if you are performing a chi-squared test with low counts.

```{r}
matrix_small_counts = matrix( c(8,11,3,22), nrow=2, ncol=2 )
matrix_small_counts
fisher.test( matrix_small_counts )
chisq.test( matrix_small_counts )
```
