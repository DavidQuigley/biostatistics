---
title: 'Module 5.2: Quantifying the confidence of an estimate'
author: "David Quigley"
output:
  word_document:
    toc: yes
  html_document:
    css: swiss.css
    toc: yes
---


```{r global_options, include=FALSE}
knitr::opts_chunk$set(comment = NA)
knitr::opts_chunk$set( prompt=TRUE ) 

knitr::knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar = c(2, 2, 1, 1))  # smaller margin on top and right
})
knitr::knit_hooks$set(left.mar = function(before, options, envir) {
    if (before) par(mar = c(2, 4, 1, 1))  # smaller margin on top and right
})
knitr::knit_hooks$set(med.mar = function(before, options, envir) {
    if (before) par(mar = c(4, 4, 1, 1))  # smaller margin on top and right
})
knitr::knit_hooks$set(bottom.mar = function(before, options, envir) {
    if (before) par(mar = c(3, 2, 1, 1))  # smaller margin on top and right
})
```


[Return to the table of contents](index.html)

********************************************************************************

Introduction
========================================

**This module explains:**

* Quantifying confidence of an estimate
    + standard error of the mean (SEM)
    + confidence intervals
* Comparing the SEM and the standard deviation


The accuracy of an estimate of the mean
================================================================================

When we estimate the mean of a population using a sample from that population, how accurate is that estimate? We can demonstrate emperically that the accuracy will depend upon

1. The variance of the population. The more variable the population is, the harder it will be to get a representative sample of any given size.
2. The size of the sample, proportionate to the total population. Larger samples are more likely to be representative of the overall population than smaller samples.

The law of large numbers
--------------------------

We can demonstrate emperically that the larger the sample is, the more accurate the sample estimate of the mean. 

As we showed earlier, for the same 1000 points generated from a normal distribution with standard deviation equal to 3 and mean equal to 15, increasing the sample size decreases the standard deviation of estimates of the mean. Each point in the figures below is an estimate of the mean, taken by randomly sampling 10, 100, or 500 of the 1000 points.

```{r echo=FALSE, fig.height=4, fig.width=8}
values = rnorm(n=1000, sd=3)+15

layout(matrix(1:3,1,3))
par(mar=c(1,3,3,1))

mus = rep(0, 100)

for(i in 1:100){
    sample_size=10
    mus[i] = mean( sample( values, size=sample_size ))
}
plot( mus, axes=FALSE, xlab="", ylab="attribute", ylim=c(12,18), 
     pch=19, col="#0000ff33", main="Sample mean estimate")
axis(2,seq(from=10, to=20, by=2), las=1)
abline( mean(values), 0)
text( 50, 13, "Sample size: 10", font=2, cex=2)
text( 50, 12.5, "Population SD: 3", font=2, cex=2)
box()

for(i in 1:100){
    sample_size=100
    mus[i] = mean( sample( values, size=sample_size ))
}
plot( mus, axes=FALSE, xlab="", ylab="attribute", ylim=c(12,18), 
     pch=19, col="#0000ff33", main="Sample mean estimate")
axis(2,seq(from=10, to=20, by=2), las=1)
abline( mean(values), 0)
text( 50, 13, "Sample size: 100", font=2, cex=2)
text( 50, 12.5, "Population SD: 3", font=2, cex=2)
box()

for(i in 1:100){
    sample_size=500
    mus[i] = mean( sample( values, size=sample_size ))
}
plot( mus, axes=FALSE, xlab="", ylab="attribute", ylim=c(12,18), 
     pch=19, col="#0000ff33", main="Sample mean estimate")
axis(2,seq(from=10, to=20, by=2), las=1)
abline( mean(values), 0)
text( 50, 13, "Sample size: 500", font=2, cex=2)
text( 50, 12.5, "Population SD: 3", font=2, cex=2)
box()
```

**Figure 1: Demonstration of the law of large numbers**

**Figure 1** shows that the sample estimate becomes more accurate as the sample size increases. We will next show that for a **constant sample size**, the standard deviation of our estimate of the mean **increases as the variance of the underlying population increases**. This should make intuitive sense, as the more variance there is in the population, the more likely it is that any particular random sample will not be representative of the overall population.

The figure below shows three distributions sampled with N=10; the distribution SD ranges from 1 to 3.

```{r echo=FALSE, fig.height=4, fig.width=8}
values1 = rnorm(n=1000, sd=1)+15
values2 = rnorm(n=1000, sd=2)+15
values3 = rnorm(n=1000, sd=3)+15

layout(matrix(1:3,1,3))
par(mar=c(1,3,3,1))

mus = rep(0, 100)

for(i in 1:100){
    sample_size=10
    mus[i] = mean( sample( values1, size=sample_size ))
}
plot( mus, axes=FALSE, xlab="", ylab="attribute", ylim=c(12,18), 
     pch=19, col="#0000ff33", main="Sample mean estimate")
axis(2,seq(from=10, to=20, by=2), las=1)
abline( mean(values1), 0)
text( 50, 13, "Sample size: 10", font=2, cex=2)
text( 50, 12.5, "Population SD: 1", font=2, cex=2)
box()

for(i in 1:100){
    sample_size=10
    mus[i] = mean( sample( values2, size=sample_size ))
}
plot( mus, axes=FALSE, xlab="", ylab="attribute", ylim=c(12,18), 
     pch=19, col="#0000ff33", main="Sample mean estimate")
axis(2,seq(from=10, to=20, by=2), las=1)
abline( mean(values2), 0)
text( 50, 13, "Sample size: 10", font=2, cex=2)
text( 50, 12.5, "Population SD: 2", font=2, cex=2)
box()

for(i in 1:100){
    sample_size=10
    mus[i] = mean( sample( values3, size=sample_size ))
}
plot( mus, axes=FALSE, xlab="", ylab="attribute", ylim=c(12,18), 
     pch=19, col="#0000ff33", main="Sample mean estimate")
axis(2,seq(from=10, to=20, by=2), las=1)
abline( mean(values3), 0)
text( 50, 13, "Sample size: 10", font=2, cex=2)
text( 50, 12.5, "Population SD: 3", font=2, cex=2)
box()
```

**Figure 2: Demonstration of the law of large numbers**

The **standard error of the mean** (also written SEM or *s.e.* or *s.e.m.*) is a random variable that can be calculated by dividing the sample standard deviation by the square root of the sample size. For a sample size *N*:

$SEM = \frac{{SD}_{sample}}{\sqrt{N}}$

Because the standard error of the mean is proportional to the square root of *N*, you need to quadruple the size of your sample to reduce the SEM by 50%. Standard error can be calculated for many statistics, so writing "standard error" is somwwhat inexact, but if you write it in the context of a test of two means it will be understood in this context.

The sample mean and SEM can be used to estimate probability that population mean lies within given bounds, given the assumption that the population has approximately normal distribution. This assumption is often met, although we will see that for small sample sizes an different distribution called the *t* distribution is appropriate.


Confidence intervals for the mean
================================================================================

Confidence intervals
-----------------------

We would like to be able to report not only our estimate of the population mean, but some information about how confident we are that our estimate reflects the true population mean. 

> If we repeated the sampling process many times, confidence interval expresses the percentage of the time the parameter should fall in the range.

Confidence intervals are reported as two values:

1. a range (on the same scale as the statistic)
2. a percentage used to generate range (typically 95% or 99%)

If our population follows a normal distribution-- or is reasonably close to one-- we can use the SEM (which is a standard deviation) to estimate bounds on where we expect the true population mean to lie. This is analgous to using the standard deviation of a normal distribution to ask how frequently we would expect to see a value 1, 2, or 3 standard deviations away from the mean. 

A typical example of reporting confidence intervals comes from (Chapman et al. NEJM 2011), the vemurafinib study introduced earlier:

>"At 6 months, overall survival was 84% (95% confidence interval [CI], 78 to 89) in the vemurafinib group and 64% (95% CI, 56 to 73)." (Chapman et al. 2011)

Calculating a confidence interval
----------------------------------

Using this logic, given the following sample:  

```
mean = 5  
standard deviation = 2  
sample N = 100  
SEM  = 2 / sqrt(100) = 0.2
```

Then we would predict that if we repeatedly sampled the mean:

* 68% within 4.8 and 5.2 (mean ± 1.0 x SE)
* 95% within 4.6 and 5.4 (mean ± 1.96 x SE)
* 99% within 4.48 and 5.52 (mean ± 2.58 x SE)

We can make this claim without knowing the true population standard deviation because of the assumption that the population has an approximately normal distribution.

The next section of R code demonstrates that confidence intervals work when the assumptions are met. The code:

1. Generates a population 100,000 values with distribution normal( mean=5, SD=2 )
2. Obtains 1,000 samples where N = 100
3. Reports how many of the 1,000 samples have means that lie within the 68%, 95%, or 99% confidence intervals

```{r}
population = rnorm( n=100000, mean=5, sd=2 )
sample_means = rep( 0, 1000 )
for( i in 1:1000 ){
     sample_means[i]= mean( sample( population )[1:100] )
}
```

```{r}
# predicted: 68%
sum( sample_means > 4.8 & sample_means < 5.2 ) / 1000
# predicted: 95%
sum( sample_means > 4.6 & sample_means < 5.4 ) / 1000
# predicted: 99%
sum( sample_means > 4.48 & sample_means < 5.52 ) / 1000
```

These bounds are called *confidence intervals*, because they describe the bounds under which we would expect our statistic to fall a given percentage of the time. **Confidence intervals are not a guarantee**. We **expect** that using confidence intervals for 2 SEM, 5% of the time the population mean will lie outside of the sample mean estimate.



Any time you report a mean value, you should also report confidence intervals. Many journals will require this; the statistical checklist for Nature journals, for example, states as requirements:

* n for each data set is clearly stated
* A clearly labeled measure of center (e.g. mean or median) is given
* A clearly label measure of variability (e.g. standard deviation or range) is given
* All numbers following a ± sign are identified as standard errors (s.e.m.) or standard deviations (s.d.)

*source: http://www.nature.com/nature/authors/gta/Statistical_checklist.doc*


Error bars
================================================================================

When we report a mean value in text, the confidence interval must be reported as well. If we plot a mean value in a figure, we must also report the confidence interval for this estimate. **Error bars** are the visual counterpart to confidence intervals.
When error bars are plotted, the figure legend must indicate the number of s.e.m. that each arm of the error bars represents.


In the following figure, a sample of 100 observations is made from a population of size 10,000. 

```{r echo=FALSE, fig.height=5, fig.width=8}

par(mar=c(2,4,1,1))
values2 = rnorm(n=10000, sd=3)+15
s = sample(values2)[1:100]
layout(matrix(1:5,1,5))

yy = c(0, 5, 10, 15, 20, 25 )
ymax=25
plot( jitter(rep(1,length(values2))), values2, ylim=c(0,ymax), axes=FALSE, 
      yaxs="i", pch=19, cex=0.2, col="cornflowerblue", ylab="", xlab="")
axis(2, yy, las=1, cex.axis=1.5 )
lines( c(-0.5, 1.5), c(mean(values2), mean(values2) ), lwd=2 )
box()
text(1,1, "A", cex=2)

plot( jitter(rep(1,length(s))), s, ylim=c(0,ymax), axes=FALSE, yaxs="i", ylab="", xlab="")
axis(2, yy, las=1, cex.axis=1.5 )
box()
text(1,1, "B", cex=2)

boxplot( s, las=1, cex.axis=1.5, ylim=c(0,ymax ),  yaxs="i" )
text(1,1, "C", cex=2)

b=barplot( mean(s), ylim=c(0,ymax ), axes=FALSE, xlim=c(0,1.5) )
axis(2, yy, las=1, cex.axis=1.5 )
box()
lines( c(b, b), c(mean(s), mean(s) + sd(s)), lwd=2 )
lines( c(b, b), c(mean(s), mean(s) - sd(s)), lwd=2 )
lines( c(b-0.2, b+0.2), c( mean(s) + sd(s), mean(s) + sd(s) ), lwd=2 )
lines( c(b-0.2, b+0.2), c( mean(s) - sd(s), mean(s) - sd(s) ), lwd=2 )
text(b,1, "D", cex=2)

SEM = sd(s) / sqrt(10)
b=barplot( mean(s), ylim=c(0,ymax ), axes=FALSE, xlim=c(0,1.5) )
axis(2, yy, las=1, cex.axis=1.5 )
box()
CIB = abs(qt( 0.32/2, df=length(s)-1))

lines( c(b, b), c(mean(s), mean(s) + CIB*SEM), lwd=2 )
lines( c(b, b), c(mean(s), mean(s) - CIB*SEM), lwd=2 )
lines( c(b-0.2, b+0.2), c( mean(s) + CIB*SEM, mean(s) + CIB*SEM ), lwd=2 )
lines( c(b-0.2, b+0.2), c( mean(s) - CIB*SEM, mean(s) - CIB*SEM ), lwd=2 )
text(b,1, "E", cex=2)

```

**Figure 3: Presenting the variation using SEM and SD**

**Figure 3A** shows the entire dataset.  
**Figure 3B** shows the sample from the dataset.  
**Figure 3C** shows a box and whiskers plot.  
**Figure 3D** shows a bar plot +/- one SD.  
**Figure 3E** shows a bar plot +/- one SEM.

We can observe:

* The SEM error bars in 3E are dramatically smaller than the SD error bars in 3D. 
* The SD error bars in 3D are much more representative of the true variation in the population. 
* The box and whiskers plot does a good job of conveying the central tendency and spread of the population.


The standard deviation *vs.* the s.e.m
================================================================================

Newcomers to statistics are often confused by the difference between the standard 
deviation and the standard error of the mean, and are unsure about which to use when reporting results.

The standard deviation describes **variation in a *sample* or *population*.**

The standard error of the mean describes **variation in an estimate of the sample or population *mean*.**

Assuming you have at least two observations in your sample, the sample s.e.m. will always be lower than the sample standard deviation. You may be tempted to report the s.e.m. instead of the standard deviation because the error bars you draw on a figure or confidence intervals you report in text will be smaller, making the observation appear more robust. However, 
to communicate how much variation there is in a sample, you need to use the standard deviation. 

The standard deviation of a whole population has a fixed value. As the size of a sample increases, the standard deviation of the *sample* may get larger or smaller. By contrast, the standard error of the mean will almost invariably decrease as the sample size increases, because *N* is in the denominator of the standard error of the mean. You can drive the sample standard error of the mean to an arbitratily low value by taking larger and larger samples. 

Whenever possible, prefer to plot the actual data. When this is impractical, plot a box-and-whiskers figure or a violin plot. The actual data can be combined with a box-and-whiskers plot to get the best of both worlds.


Summary
===========

* Standard Error of the Mean (SEM) depends on 
    + population variance
    + sample size
* SEM is a standard deviation of the sample corrected for the square root of the number of samples

* Express the Confidence Interval (CI) using bounds and a percentage

* If we repeated the sampling process many times, the CI expresses the percentage of the time the parameter should fall in the range.

* Do not use SEM to communicate the variation in the population.



