---
title: 'Module 5.4: The t test: comparing the mean between groups'
author: "David Quigley"
output:
  word_document:
    toc: yes
  html_document:
    css: swiss.css
    toc: yes
---

```{r echo=FALSE}
quick.t.plot = function( X, Y ){
    plot( c( rep(1, length(X)), rep(2, length(Y)) ),
         c( X, Y),
      xlim=c(0.5,2.5), ylim=c(0,10), pch=19, ylab="", xlab="", axes=FALSE)
    axis(1, at=c(1,2), labels=c("group 1", "group 2"), tick=FALSE)
    axis(2, at=c(0, 2,4,6,8,10), las=1)
    box()
}
```

Introduction
========================================

**This module explains:**

* Using the *t* test to compare the mean between groups
    + unpaired groups
    + when group members are paired
* calculating a P value from the *t* statistic



The t test
===============================================================

We frequently wish to test whether two samples are likely to have come from distinct populations. For example:

* Do mice of the C57BL/6 strain run on a wheel more hours per night than FVB/N mice?
* Patients in a clinical trial were randomized to receive drug or a placebo. Do subjects receiving the drug have a lower blood pressure than when they started?
* Did tenured scientists whose PhD was supervised by a member of the National Academy of Science receive their first RO1 at a younger age than those who were not supervised by a NAS member?

These situations compare the value of a continuous variables from two distinct groups. The question of interest is whether the sampled group means are far enough apart that we think they are unlikely to have been derived from a common distribution. 

The following data summarize the age of admission for patients who were discharged from New York hospitals after a myocardial infarction, commonly known as a heart attack. I acknowledge and thank the [UCLA SOCR](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_AMI_NY_1993_HeartAttacks) for making these data available.

```{r echo=FALSE,  fig.height=4, fig.width=4}
fn='/notebook/biostat_course/supporting_documents/1993_heart_attack.txt'
heart = read.table(fn, sep='\t', header=TRUE, stringsAsFactors=FALSE)
par(mar=c(3,3,2,1))
hist(heart$AGE, xlab="patient age", main="", col="cornflowerblue")
box()
```

**Figure 1: age of patient discharge dichotomized by sex**

We can ask whether men tended to be admitted at the same age as women.

```{r echo=FALSE, fig.height=4, fig.width=8}
layout(matrix(1:2,1,2))
par(mar=c(3,3,2,1))
hist(heart$AGE[heart$SEX=="M"],  xlab="patient age", main="Male", col="cornflowerblue")
box()
hist(heart$AGE[heart$SEX=="F"], xlab="patient age", main="Female", col="cornflowerblue")
box()
```

**Figure 2: age of patient discharge dichotomized by sex**

The mean age of discharge for men is `r round( mean(heart$AGE[heart$SEX=="M"]), 2)`, while for women the value is `r round( mean(heart$AGE[heart$SEX=="F"]), 2)`. It certainly appears that women tend to be older when they are discharged, implying that men get heart attacks at a younger age.  

A statistical test is required to quantify whether this difference in means would be expected by chance.

The simplest way to evaluate the statistical strength of evidence for this form of question is a *t* test.  If we have exactly two groups defined by a categorical variable, then the *t* test is often appropriate. The null hypothesis of the *t* test is that both samples are random samples from the same population. The **alternative hypothesis** is that there are two distinct populations. The *t* test produces a *t* statistic; that is, a statistic that has a *t* distribution. The farther the *t* statistic is from zero, the less likely it is that the two samples came from the same original distribution.

The *t* statistic is a ratio: roughly speaking, it measures the difference in means of two samples, divided by sample variance, normalized by the number of samples. Stated more formally, *t* for two samples is the difference in sample means divided by the sum of squared standard deviations, with each group's standard deviation normalized by the number of samples in that group. The denominator of this equation may remind you of the formula for the standard error of the mean; that's no accident. A *t* statistic is proportional to how different two means are, adjusted for the relative variance in values contributing to each estimate of the mean.

$\large{t = \frac{ \bar{Y_1} - \bar{Y_2}} { \sqrt{ \frac{s_1^2}{N_1} + \frac{s_2^2}{N_2} } } }$

Recall that $\bar{Y_1}$ is the mean of group $Y_1$ and $s_1^2$ is the variance of group 1. As the difference in means gets smaller, the *t* statistic will get closer to zero. Since the *N* values are the denominator of a denominator, larger N values will tend to *increase* the magnitude of the *t* statistic by decreasing the variance term in the denominator. This makes sense intuitively: the more samples we have, the more certain we can be of our statistic.

The formulation of the *t* statistic shown above is appropriate if the variance of the two groups may differ. This test is also called "Welsh's *t* test". 

```{r echo=FALSE, fig.height=3, fig.width=3}
par(mar=c(3,3,1,1))
boxplot( heart$AGE~heart$SEX, las=1,col="cornflowerblue", names=c("female", "male"), ylab="years")
```

**Figure 3: box and whiskers plots for male and female discharge ages**

The *t* statistic for this test is very, very low: -35.98. The *P* value is correspondingly astronomically low.

Many real data sets are much smaller.

```{r echo=FALSE, fig.height=4, fig.width=3}
X = c(7.9,6.3,5.0,6.7,9.7,6.0,6.8,6.5,5.8,5.9,7.0,7.5,5.6,5.7,6.1,8.1,5.7,4.9,5.5,6.2)
Y = c(5.8,4.2,2.9,4.3,8.1,3.7,4.7,4.4,3.7,3.7,5.1,5.4,3.4,3.7,3.9,6.3,3.7,2.9,3.5,4.1)

par(mar=c(3,3,1,1))
quick.t.plot(X,Y)
text(1.5,1.5, paste("t =", signif( t.test(X,Y)$statistic, 3)), cex=1.5) 
text(1.5,0.5, paste("P =", signif( t.test(X,Y)$p.value, 3)), cex=1.5) 
group_1 = X
group_2 = Y
ts = t.test( group_1, group_2 )
```

**Figure 4: a smaller example**

Here I show two samples where *N* for both groups is 20. The *t* test returns the following results:

```{r}
t.test( group_1, group_2 )
```

The t statistic is `r round( ts$statistic, 3)`, which produces a P value of $`r signif( ts$p.value, 3)`$ when tested on a *t* distribution with `r round(ts$parameter,2)` degrees of freedom. It may seem odd that the number of degrees of freedom is not a round number. There are 40 independent values and two means, so knowing 19 values in each group you can infer the 20th value. However, because of allowance for unequal variance, the number of degrees of freedom is calculated by an equation (not worth memorizing) called the Welsh-Satterthwaite equation:

$\large{df = \frac{ ( \frac{s^2_1}{N_1} + \frac{s^2_2}{N_2} )^2 }{ \frac{ (\frac{s^2_1}{N_1})^2 }{(N_1-1)} + \frac{ (\frac{s^2_2}{N_2})^2 }{(N_2-1)} }}$

If both samples are assumed to have the same variance, a slightly different equation is used to calculate the *t* statistic:

$\large{t = \frac{ \bar{Y_1} - \bar{Y_2}} {  S_{X_1X_2} \times \sqrt{ \frac{1}{N_1} + \frac{1}{N_2}} } }$

This equation uses the pooled variance of both samples; the number of degrees of freedom is $N_1 - 1$ + $N_2 - 1$.

If we ask R to assume the variances are equal, this changes the the number of degrees of freedom and results in a slightly lower *P* value.

```{r}
t.test( group_1, group_2, var.equal=TRUE )
```

One-tailed and two-tailed *P* values
=====================================

Recall that the P value represents the area under the *t* distribution as far as or farther from zero than a given *t* statistic.

```{r echo=FALSE}
par(mar=c(4,4,3,1))
vals = seq(from=-5,to=5, by=0.01)
layout(matrix(1:2,1,2))
plot( vals, dt(vals, df=37.74), xlab="", ylab="density", xaxs="i",yaxs="i",las=1, type="l", lwd=3, ylim=c(0,0.45), main="t distribution" )

for( i in seq(from=2, to=5, by=0.01)){
    lines( c(i,i), c(0, dt(i, df=37.74)), col="cornflowerblue", lwd=2)
}

points( vals, dt(vals, df=37.74), type="l", lwd=3 )


plot( vals, dt(vals, df=37.74), xlab="", ylab="density", xaxs="i",yaxs="i",las=1, type="l", lwd=3, ylim=c(0,0.45), main="t distribution" )
for( i in seq(from=2, to=5, by=0.01)){
    lines( c(i,i), c(0, dt(i, df=37.74)), col="cornflowerblue", lwd=2)
}
for( i in seq(from=-5, to=-2, by=0.01)){
    lines( c(i,i), c(0, dt(i, df=37.74)), col="cornflowerblue", lwd=2)
}
points( vals, dt(vals, df=37.74), type="l", lwd=3 )
```

**Figure 5: one and two-tailed distributions**

The left side of **Figure 5** is a *t* distribution with 37.74 DF, *t* = 2. The *P* value for this *t* statistic is 0.026. Note that only one side of the distribution has a blue area filled in; this P value was calculated from only one tail of the distribution.

The plot on the right has both tails filled in at +/- 2. This is equivalent to a test asking whether the mean of two groups are different from each other, allowing for either group mean to be larger. **Generally, this is the correct test to run even though the *P* value will be twice as large**. The *P* value for the distribution on the right is 0.052.

You need a very good reason to justify the use of a single-tailed test; unless you have clear rationale, use the two-tailed test.

Paired data
-------------------

The version of the *t* test we have discussed so far assumes that individual data points in the samples are statistically independent. This is not always the case. Some examples:

* Number of lymphocytes in a breast tumor sample *vs.* adjacent normal tissue from the same woman
* Blood pressure of the same individual before *vs.* after drug administration
* Number of hours mice spend running on a wheel at day *vs.* at night

In each of these cases, one of the values will be linked to the other because the same individual is being sampled twice. In statistical parlance this is a "repeated measure." If data were derived from a samples where values in group 1 are linked to a corresponding value in group 2, then it is important to perform a **paired *t* test** rather than an unpaired *t* test. The paired *t* test links each value in group 1 with a corresponding value in group 2. This means that the *t* statistic depends on the pairing. In **Figure 6** I plot the same points with two different pairings; note the *P* values for each test.

```{r echo=FALSE, fig.height=3, fig.width=7}
layout(matrix(1:2,1,2))
par(mar=c(3,3,1,1))
X = c(7.9,6.3,5.0,6.7,9.7,6.0,6.8,6.5,5.8,5.9,7.0,7.5,5.6,5.7,6.1,8.1,5.7,4.9,5.5,6.2)
Y = c(5.8,4.2,2.9,4.3,8.1,3.7,4.7,4.4,3.7,3.7,5.1,5.4,3.4,3.7,3.9,6.3,3.7,2.9,3.5,4.1)
quick.t.plot(X,Y)
for( i in 1:length(X)){
    lines(c( 1,2), c(X[i], Y[i] ) )
}
text(2,0.5, paste("P =", signif( t.test(X,Y, paired=TRUE)$p.value, 3)), adj=1) 

idx = sample(1:20)
quick.t.plot(X,Y[idx])
for( i in 1:length(X)){
    lines(c( 1,2), c(X[i], Y[idx][i] ) )
}
text(2,0.5, paste("P =", signif( t.test(X,Y[idx], paired=TRUE)$p.value, 3)), adj=1) 

```

**Figure 6: Two different pairings of a set of values**

For a paired *t* test, one calculates the differences between the two groups *D*. Normally the paired *t* statistic is calculated as

$\large{t = \frac{ \bar{D}}{ \frac{s_D}{\sqrt(N)} } }$

with N-1 degrees of freedom. If you wish to test whether the paired differences are significantly different from some other value than zero, the test statistic is calculated using that value *V* as 

$\large{t = \frac{ \bar{D} - V}{ \frac{s_D}{\sqrt(N)} } }$


Summary
================


* The t test evaluates the difference between two sample means
* Two ways to deal with variation in samples:
    + pooled variance
    + allow variance to be different in each sample
* Use two-tailed comparisons unless you have an excellent justification for using a single-tailed test 


R code examples
=======================

> The t distribution value for a given number of degrees of freedom

```{r}
t_value = 3
degrees_of_freedom=5
dt(t_value, df=degrees_of_freedom)

```

> The t test

I'll show some t tests of the following data:

```{r fig.width=3, fig.height=4, echo=FALSE}
group_1 = c(3,6,7,2,1,4,4.5, 6)
group_2 = c(5,4,12,6,7,6,8,9)
par(mar=c(3,3,1,1))
quick.t.plot(group_1, group_2)
```

```{r}
group_1 = c(3,6,7,2,1,4,4.5, 6)
group_2 = c(5,4,12,6,7,6,8,9)
t.test(group_1, group_2)
```

It is usually correct to perform a two-sided *t* test; that is, to test the alternative hypothesis that group 2 is either larger *or* smaller than group 1. If you have a very well-motivated reason, you can test a one-sided hypothesis using the *alternative* parameter, passing in either "less" or "greater". Note that the "less" refers to the first group:

```{r}
group_1 = c(3,6,7,2,1,4,4.5, 6)
group_2 = c(5,4,12,6,7,6,8,9)
t.test(group_1, group_2, alternative="less")
```

Note that the p-value reported by the *t.test()* function has dropped because only one side of the t-distribution is being considered in the "as extreme or more extreme" calculation. 

To test paired data, set the *paired* parameter to true. This requires that you have the same number of samples in each group:

```{r}
group_1 = c(3, 6, 7, 2, 1, 4, 4.5, 6)
group_2 = c(5, 4, 12,6, 7, 6, 8, 9)
t.test(group_1, group_2, paired=TRUE)
```
